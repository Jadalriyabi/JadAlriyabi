{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#346efe; border-radius: 100px 100px; text-align:center\">Torch inference notebook</h1></span>","metadata":{}},{"cell_type":"markdown","source":"<br>\n<h2 style = \"font-size:16px\" \n\nThis is the inference notebook made for training with  https://www.kaggle.com/vladvdv/pytorch-train-notebook-arcface-gem-pooling/notebook  \n    \nModifications for version 30:\n* replaced supervized KNeighborsClassifier with unsupervized NearestNeighbors   \n* corrected gridsearch for determining optim \"new_individual\" threhsold* (there are used the same training data as the ones the model was trained, for training the NearestNeighbors algorithm, and then the same validation data that the model was trained to predict on the NearestNeighbors algorithm.  \n   \nTo do:\n* Implement all folds model blending","metadata":{}},{"cell_type":"code","source":"import pickle\nimport os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sys\nsys.path.append(\"../input/timm-pytorch-image-models\")\nimport timm\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestNeighbors","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-05T21:31:26.642055Z","iopub.execute_input":"2022-04-05T21:31:26.642392Z","iopub.status.idle":"2022-04-05T21:31:26.651533Z","shell.execute_reply.started":"2022-04-05T21:31:26.64235Z","shell.execute_reply":"2022-04-05T21:31:26.650826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"seed\": 21, # choose your lucky seed\n          \"img_size\": 768, # training image size\n          \"model_name\": \"tf_efficientnet_b4\", # training model arhitecture\n          \"num_classes\": 15587, # total individuals in training data\n          \"test_batch_size\": 16, # choose acording to the training arhitecture and image size \n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), # gpu\n          \"test_mode\":False, # selects just the first 2000 samples from the test data, usefull for debuging purposes\n          \"percentage_new_from_test\":10, # how much of the test data is estimated to be \"new_individual\"\n          \"threshold\":0.1, # it will be overwriten after prediction histogram           \n          \"neigh\":100, #cnn neighbors \n          \"n_fold\":5, # nr of folds that the model has been trained\n          # ArcFace Hyperparameters\n          \"s\": 30.0, \n          \"m\": 0.30,\n          \"ls_eps\": 0.0,\n          \"easy_margin\": False\n          }","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:33.144557Z","iopub.execute_input":"2022-04-05T21:31:33.145118Z","iopub.status.idle":"2022-04-05T21:31:33.1938Z","shell.execute_reply.started":"2022-04-05T21:31:33.145078Z","shell.execute_reply":"2022-04-05T21:31:33.192995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:35.114876Z","iopub.execute_input":"2022-04-05T21:31:35.115611Z","iopub.status.idle":"2022-04-05T21:31:35.124325Z","shell.execute_reply.started":"2022-04-05T21:31:35.115572Z","shell.execute_reply":"2022-04-05T21:31:35.123433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_file_path(id):\n    return f\"{TEST_DIR}/{id}\"\n\ndef get_train_file_path(id):\n    return f\"{TRAIN_DIR}/{id}\"\n\nROOT_DIR = '../input/happy-whale-and-dolphin'\nTEST_DIR = '../input/happy-whale-and-dolphin/test_images'\nTRAIN_DIR = '../input/happy-whale-and-dolphin/train_images'\n# weights_path = \"../input/dummymodel13/Loss11.1684_epoch13_f0.bin\"\n\nif CONFIG[\"test_mode\"]==True:\n    df_test = pd.read_csv(f\"{ROOT_DIR}/sample_submission.csv\")[:2000]\n    df_train = pd.read_csv(f\"{ROOT_DIR}/train.csv\")[:2000]\nelse:\n    df_test = pd.read_csv(f\"{ROOT_DIR}/sample_submission.csv\")\n    df_train = pd.read_csv(f\"{ROOT_DIR}/train.csv\")  \n\n\n\ndf_test['file_path'] = df_test['image'].apply(get_test_file_path)\ndf_train['file_path'] = df_train['image'].apply(get_train_file_path)\ntrain_labels = np.array(df_train['individual_id'].values)\n\n#split into train and valid like in the training notebook for validating NearestNeighbors approach \n# trainFold = 0 # this model was trained on fold 0\n# skf = StratifiedKFold(n_splits=CONFIG['n_fold'])\n# for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=train_labels)):\n#       df_train.loc[val_ , \"kfold\"] = fold\n# df_train_cnn = df_train[df_train.kfold != trainFold].reset_index(drop=True)\n# df_valid_cnn = df_train[df_train.kfold == trainFold].reset_index(drop=True)\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(df_train['individual_id'])\ndf_train['dummy_labels'] = le.transform(df_train['individual_id'])\ndf_test['dummy_labels'] = 0\nn_classes = max(df_train['dummy_labels'].values)+1\nprint(n_classes)\n\n# print(df_train['file_path'])\n\n#hardcode dummy label for input in ArcMargin forward function\n# df_test['dummy_labels'] = 0\n# df_train_cnn['dummy_labels'] = 0\n# df_valid_cnn['dummy_labels'] = 0\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:36.465304Z","iopub.execute_input":"2022-04-05T21:31:36.465576Z","iopub.status.idle":"2022-04-05T21:31:36.668395Z","shell.execute_reply.started":"2022-04-05T21:31:36.465548Z","shell.execute_reply":"2022-04-05T21:31:36.667628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HappyWhaleDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df['dummy_labels'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n        return img_path, img, torch.tensor(label, dtype=torch.long)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:38.089245Z","iopub.execute_input":"2022-04-05T21:31:38.090047Z","iopub.status.idle":"2022-04-05T21:31:38.099395Z","shell.execute_reply.started":"2022-04-05T21:31:38.090011Z","shell.execute_reply":"2022-04-05T21:31:38.098687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\n\n\nclass ClassifierModule(nn.Module):\n    def __init__(self):\n        super(ClassifierModule,self).__init__()\n        self.layer1 = nn.Linear(1000,n_classes)\n        self.net = models.resnet18()\n        for p in self.net.parameters():\n            p.requires_grad=True\n\n    def forward(self,x):\n        x1 = self.net(x)\n        y = self.layer1(x1)\n        return y\n\nmodel = ClassifierModule()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:39.330307Z","iopub.execute_input":"2022-04-05T21:31:39.330786Z","iopub.status.idle":"2022-04-05T21:31:39.682578Z","shell.execute_reply.started":"2022-04-05T21:31:39.330746Z","shell.execute_reply":"2022-04-05T21:31:39.681819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:44.713872Z","iopub.execute_input":"2022-04-05T21:31:44.714583Z","iopub.status.idle":"2022-04-05T21:31:44.719911Z","shell.execute_reply.started":"2022-04-05T21:31:44.714545Z","shell.execute_reply":"2022-04-05T21:31:44.719197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \"test\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n    \n}","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:46.625695Z","iopub.execute_input":"2022-04-05T21:31:46.626273Z","iopub.status.idle":"2022-04-05T21:31:46.631693Z","shell.execute_reply.started":"2022-04-05T21:31:46.626231Z","shell.execute_reply":"2022-04-05T21:31:46.630975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.to(CONFIG['device']);\n#predict first on train dataset to extract embeddings\ntrain_dataset = HappyWhaleDataset(df_train, transforms=data_transforms[\"test\"])\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG['test_batch_size'], \n                          num_workers=4, shuffle=True, pin_memory=True)\n\n# valid_dataset = HappyWhaleDataset(df_valid, transforms=data_transforms[\"test\"])\n# valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['test_batch_size'], \n#                           num_workers=4, shuffle=False, pin_memory=True)\n\ntest_dataset = HappyWhaleDataset(df_test, transforms=data_transforms[\"test\"])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'], \n                          num_workers=4, shuffle=False, pin_memory=True)\n\n# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\noptimizer = torch.optim.Adam(params=list(model.parameters()), lr=5e-4, betas=(0.9, 0.999))\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:31:52.399805Z","iopub.execute_input":"2022-04-05T21:31:52.400375Z","iopub.status.idle":"2022-04-05T21:31:55.373917Z","shell.execute_reply.started":"2022-04-05T21:31:52.400335Z","shell.execute_reply":"2022-04-05T21:31:55.373262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/working/ckpt.pt\"","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:32:31.746944Z","iopub.execute_input":"2022-04-05T21:32:31.747526Z","iopub.status.idle":"2022-04-05T21:32:32.023581Z","shell.execute_reply.started":"2022-04-05T21:32:31.747487Z","shell.execute_reply":"2022-04-05T21:32:32.022705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ONLY RUN IF CHECKPOINT NEEDS TO BE LOADED\n\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T21:44:22.84819Z","iopub.execute_input":"2022-04-05T21:44:22.84881Z","iopub.status.idle":"2022-04-05T21:44:22.943849Z","shell.execute_reply.started":"2022-04-05T21:44:22.848769Z","shell.execute_reply":"2022-04-05T21:44:22.943058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\nfor epoch in range(40):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        _, inputs, labels = data\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n#         print(labels)\n#         print(inputs.shape)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        niter_print = 500\n        if i % niter_print == niter_print-1:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / niter_print:.9f}')\n            running_loss = 0.0\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                }, PATH)\n\nprint('Finished Training') ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:05:06.110851Z","iopub.execute_input":"2022-04-04T20:05:06.112367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing\nsft = nn.Softmax()\nmodel.eval()\n\nimport csv  \n\nheader = ['image', 'predictions']\n\nsf = open('countries.csv', 'w', encoding='UTF8')\nwriter = csv.writer(sf)\nwriter.writerow(header)\n    \nfor i, data in enumerate(test_loader, 0):\n    # get the inputs; data is a list of [inputs, labels]\n    path, inputs, labels = data\n    inputs = inputs.cuda()\n    labels = labels.cuda()\n\n    # forward\n    outputs = sft(model(inputs))\n    \n    tk = torch.topk(outputs, 4).indices\n    tklbls = le.inverse_transform(tk.view(-1).cpu()).reshape(tk.shape)\n    for idx, p in enumerate(path):\n        opt = p.split('/')[-1]+\",\"+tklbls[idx][0]+\" \"+tklbls[idx][1]+\" \"+tklbls[idx][2]+\" \"+tklbls[idx][3]+\" new_individual\"\n        print(opt)\n        writer.writerow([p.split('/')[-1], tklbls[idx][0]+\" \"+tklbls[idx][1]+\" \"+tklbls[idx][2]+\" \"+tklbls[idx][3]+\" new_individual\"])\nsf.close()\nprint(\"DONE!\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T14:51:23.015201Z","iopub.execute_input":"2022-04-05T14:51:23.015621Z","iopub.status.idle":"2022-04-05T14:51:23.124991Z","shell.execute_reply.started":"2022-04-05T14:51:23.015523Z","shell.execute_reply":"2022-04-05T14:51:23.123428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/ckpt.pt\"\n\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, PATH)\n\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}